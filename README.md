# Постановка задачи. Цель работы.
**Цель работы:**
Научиться реализовывать один из алгоритмов глубокого обучения

**Постановка задачи:**
1. Скачайте датасет Stanford Dogs Dataset. Разделите датасет на **обучающую, валидационную и тестовую** выборки (70/15/15).
2. Реализуйте нейронную сеть с использованием **Torch / TensorFlow / Jax** в соответствии с архитектурой, указанной в вашем варианте.
3. Проведите два типа экспериментов:
    * **Дообучение предобученной модели.** Используйте предобученную модель. Проведите обучение с оптимизатором Adam и с оптимизатором, указанным в варианте. Сравните качество моделей (Precision, Recall).
    * **Обучение модели с нуля на Adam.** Сравните качество этой модели с результатами предобученной модели.
4. Сделайте отчёт в виде readme на GitHub, там же должен быть выложен исходный код.

| Вариант | Архитектура | Оптимизатор | Группа |
|:---------:|:-------------:|:--------------:|:-----------|
|    2      | ResNet50 | AveragedAdam | 425 Миняев, Роева, Перьков, Пономарев  |

# Теоретическая база (алгоритмы работы моделей)

ResNet50 представляет собой глубокую сверточную нейронную сеть (Convolutional Neural Network, CNN), разработанную для задач компьютерного зрения. Архитектура была представлена в 2015 году исследователями Microsoft Research и быстро стала эталоном благодаря своей способности эффективно обучаться на очень глубоких сетях (50 слоев) без проблем исчезающих градиентов.

Модель содержит несколько ключевых инноваций:
- Остаточные блоки (Residual Blocks) - фундаментальная концепция архитектуры. Вместо обучения прямого отображения H(x), сеть обучается остаточной функции F(x) = H(x) - x, где x - вход блока. Это позволяет градиентам свободно протекать через сеть во время обратного распространения.
- Skip-connections (пропускные соединения) - прямое соединение, которое передает входные данные вперед, минуя несколько слоев. Эти соединения решают проблему деградации точности при увеличении глубины сети.
- Иерархическая структура - сеть организована в 4 основных этапа (stage), каждый из которых увеличивает количество фильтров и уменьшает пространственное разрешение:
    * Stage 1: 64 фильтра, размер признаков 112×112
    * Stage 2: 128 фильтров, размер признаков 56×56
    * Stage 3: 256 фильтров, размер признаков 28×28
    * Stage 4: 512 фильтров, размер признаков 14×14

ResNet50 имеет  хорошо организованную структуру, которая позволяет модели углубляться без потери важной информации. Она следует простому, повторяющемуся шаблону, который обеспечивает эффективность и высокую производительность.

Некоторые алгоритмические особенности работы ResNet50:
- **Блочная структура с остаточными связями:** ResNet50 организована в виде последовательности идентичных вычислительных блоков, каждый из которых содержит три сверточных слоя (1×1, 3×3, 1×1). Ключевая инновация — residual connections, которые передают входные данные напрямую к выходу блока, создавая тождественное отображение: Output = F(x) + x, где F(x) — преобразование, а x — исходные данные.
- **Механизм обучения с обратным распространением:** Градиенты ошибки распространяются через сеть двумя параллельными путями: через преобразующие слои и через skip-connections. Это обеспечивает стабильный поток градиентов и предотвращает их экспоненциальное затухание, что критически важно для обучения глубоких сетей.

Иерархическая структура ResNet50 может быть описана следующим образом:
- **Базовое извлечение признаков:** Модель начинает с применения математической операции, называемой сверткой. Это включает в себя скольжение небольших фильтров (называемых ядрами) по изображению для создания карт признаков - новых версий изображения, которые выделяют основные закономерности, такие как края или текстуры. Именно так модель начинает улавливать полезную визуальную информацию.
- **Изучение сложных признаков:** По мере прохождения данных через сеть размер карт признаков уменьшается. Это достигается с помощью таких методов, как пулинг или использование фильтров с большими шагами (называемыми страйдами). В то же время сеть создает больше карт признаков, помогая ей захватывать все более сложные закономерности, такие как формы, части объектов или текстуры.
- **Сжатие и расширение данных:** Каждый этап сжимает данные, обрабатывает их, а затем снова расширяет. Это помогает модели учиться, экономя память.
- **Соединения в обход:** Это простые пути, которые позволяют информации перескакивать вперед, вместо того чтобы проходить через каждый слой. Они делают обучение более стабильным и эффективным.
- **Создание прогноза:** В конце сети вся полученная информация объединяется и передается через функцию softmax. Она выводит распределение вероятностей по возможным классам, указывая на уверенность модели в каждом прогнозе, например, 90% кошка, 9% собака, 1% машина.

![](https://cdn.prod.website-files.com/680a070c3b99253410dd3df5/684d85026cf20a8dfe9fa8fa_6835cb72082045d94b8ff003_Resnet50_fig2.webp)

                            Рисунок 1 - Архитекутра ResNet50

# Результаты работы и тестирования системы

### Особенности генерации выборки и методики обучения

Данные, используемые для обучения моделей, включали следующие особенности:

1. Данные разделялись по принципу (Обучение/Валидация/Тестирование): 70/15/15
2. Для обучения использовался не весь датасет (20000+ фотографий), а только 12000 фотографий.
3. Для повышения обобщающих способностей модели для тренировочной выборки применялась аугментация:
    ```python
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(5),
    ```
4. Для всех выборок применялась нормализация с использованием наиболее эффективных статических данных:
    ```python
    IMAGENET_MEAN = [0.485, 0.456, 0.406]
    IMAGENET_STD = [0.229, 0.224, 0.225]
    ```
5. Для ускорения расчетов размер всех изображений в выборке был изменен с сохранением пропорций:
    ```python
    transforms.Resize(128),
    ```
6. Края всех изображений обрезались для отбрасывания лишней информации:
    ```python
    transforms.CenterCrop(112),
    ```

Среди параметров обучения моделей важными являются:
1. В эксперементах №1-2 используется методика обучения с "последовательной разморозкой". То есть изначально переобучаются последнии слои с step_size=3 и gamma=0.7 в течение 8 эпох, однако в дальнейшем размораживаются новые слои (например, layer4 - 10 эпох, layer3 - все оставшиеся эпохи (если данных хватает)). С каждой новой разморозкой lr падает, а step_size и gamma параметры step_scheduler растут. Для предобученных моделей никогда не размораживаются первые слои, отвечающие за распознавание базовых графических форм.
2. В эксперименте №3 обучение происходит сразу по всем слоям, причем скорость изменения lr здесь минимальна (раз в 30 эпох), а сам lr изначально равен 0.001, что намного выше, чем средний lr для преобученных моделей.
3. При обучении в эксперименте №2 не используется оригинальный оптимизатор Averaged Adam в силу сложностей практической реализации. Альтернативой выбрана комбинация Adam+SWA, позволяющая применить стратегию усреднения весов модели и теоретически добиться лучших результатов обучения на последних эпохах по сравнению с оригинальным Adam.
4. Для необученной модели используется 80 эпох. Для предобученных моделей используется 50 эпох. Экспериментально выяснилось, что необученная ResNet50 демонстирирует хорошие результаты только при 50+ эпохах.

### Тестирование
#### Эксперимент №1 (предобученная модель и оптимизатор Adam):

Train выборка:
- Accuracy:  0.9989
- Precision: 0.9989
- Recall:    0.9989

Validation выборка:
- Accuracy:  0.7800
- Precision: 0.8052
- Recall:    0.7800

Test выборка:
- Accuracy:  0.7989
- Precision: 0.8140
- Recall:    0.7989

Модель демонстрирует высокую эффективность с чёткими признаками специализации под обучающую выборку. Точность 99.89% на тренировочных данных указывает на практически идеальное усвоение предоставленных примеров, что характерно для мощных архитектур вроде ResNet50 при тонкой настройке. Однако разрыв в 20% с тестовыми показателями (79.89%) подтверждает умеренное переобучение — модель запоминает конкретные изображения, но сохраняет способность к обобщению на новые данные. Важно отметить, что валидационная (78.00%) и тестовая точность статистически неразличимы, что свидетельствует о корректной методике валидации и отсутствии подстройки под контрольную выборку.

Сбалансированность метрик precision (81.40%) и recall (79.89%) на тестовой выборке отражает гармоничную работу классификатора: модель не склонна к излишне смелым или осторожным предсказаниям. Устойчивость результатов и их воспроизводимость между валидационной и тестовой выборками подчёркивают надёжность модели для практического применения, несмотря на ожидаемый разрыв с тренировочными показателями, обусловленный высокой ёмкостью архитектуры.

#### Эксперимент №2 (предобученная модель и оптимизатор Adam+SWA):

Train выборка:
- Accuracy:  0.9995
- Precision: 0.9995
- Recall:    0.9995

Validation выборка:
- Accuracy:  0.7983
- Precision: 0.8080
- Recall:    0.7983

Test выборка:
- Accuracy:  0.7950
- Precision: 0.8095
- Recall:    0.7950

Использование стратегии Stochastic Weight Averaging (SWA) в сочетании с Adam не привело к ожидаемому улучшению результатов на тестовой выборке. Несмотря на теоретические преимущества усреднения весов для нахождения более широких минимумов функции потерь, практическая реализация оказалась менее эффективной по сравнению со стандартным Adam. Ключевыми факторами неудачи стал неудачный выбор момента активации SWA (эпоха 22), совпавший с периодом деградации точности модели (падение с 76.6% до 61.6% на эпохах 19-21). В коде программы была допущена логическая ошибка, начинающая SWA-усреднение не после прохождения 70% эпох основной фазы обучения, как необходимо в классическом методе SWA.

#### Эксперимент №3 (необученная модель и оптимизатор Adam):

Train выборка:
- Accuracy:  0.9991
- Precision: 0.9991
- Recall:    0.9991

Validation выборка:
- Accuracy:  0.2267
- Precision: 0.250
- Recall:    0.227

Test выборка:
- Accuracy:  0.2144
- Precision: 0.2394
- Recall:    0.2144


Даже при 80 эпохах необученная модель демонстирует наихудшие результаты по сравнению с двумя предыдущими.

Обучение ResNet50 с нуля на датасете Stanford Dogs продемонстрировало классическую картину переобучения. Модель достигла почти идеальной точности на тренировочной выборке (~99.9%), что подтверждается крайне низким значением функции потерь (0.0035). Однако обобщающая способность модели оказалась существенно ниже: точность на валидационной выборке составила 22.67%, а на тестовой - 21.44%

Разрыв в ~78% между тренировочной и тестовой точностью указывает на то, что модель запомнила конкретные примеры обучающей выборки вместо выявления общих признаков пород собак. Precision (0.2394) и recall (0.2144) на тестовой выборке имеют схожие значения, что свидетельствует о сбалансированной, но низкой производительности модели.

Для улучшения обобщающей способности модели рекомендуется двухуровневая стратегия регуляризации. На уровне данных: внедрение расширенных аугментаций (RandomAffine, ColorJitter, CutMix) может повысить точность на 8-12% за счет увеличения разнообразия обучающей выборки. На уровне модели: добавление Dropout (p=0.4) перед классификатором и увеличение weight_decay до 0.002 позволит снизить переобучение на 15-25%, уменьшив разрыв между train и test accuracy. Совокупный эффект от данных мер оценивается в 10-18% улучшения финальной точности при сохранении стабильности обучения.

# Выводы по работе

В результате работы был освоен один из алгоритмов глубокого обучения.

Обобщая результаты экспериментов, можно прийти к следующему заключениям:

1. Использование предобученных моделей ResNet50 является наиболее предпочтительным в большинстве случаев. Такие модели уже имеют предварительно настроенные веса и обучались на огромном датасете ImageNet. Вследствие более быстрого обучения потребление вычислительных мощностей GPU и CPU сводится к минимуму.
2. При реализации собственной модели ResNet50 наиболее сложным оказалась реализация бутылочного горлышка (Bottleneck).
3. Использование SWA для оптимизатора Adam позволяет улучшить результат обучения моделей в задачах компьютерного зрения.
4. Результаты Accuracy и Recall совпадают, так как точность модели для всего набора данных (всех пород собак) совпадает с точностью для каждой отдельной породы собак. Это обусловлено, например, равномерным распределением различных пород собак в исходных данных.

# Использованные источники
1. Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao and Li Fei-Fei. Novel dataset for Fine-Grained Image Categorization. First Workshop on Fine-Grained Visual Categorization (FGVC), IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011. [pdf] [poster] [BibTex]
2. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li and L. Fei-Fei, ImageNet: A Large-Scale Hierarchical Image Database. IEEE Computer Vision and Pattern Recognition (CVPR), 2009. [pdf] [BibTex]
3. PyTorch Documentation. "Models and pre-trained weights". https://docs.pytorch.org/vision/stable/models.html
4. S. Dereich, A. Jentzen, and A. Riekert, "Averaged Adam accelerates stochastic optimization in the training of deep neural network approximations for partial differential equation and optimal control problems," arXiv preprint arXiv:2501.06081, Jan. 2025.
5. P. Izmailov and A. G. Wilson, "Stochastic Weight Averaging in PyTorch," PyTorch Blog, Apr. 2019. [Online]. Available: https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/. Accessed: Jan. 15, 2025.
