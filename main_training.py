import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.swa_utils import AveragedModel
from torch.utils.data import DataLoader, random_split, Subset
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import torchvision.models as models
import numpy as np
from sklearn.metrics import precision_score, recall_score
import os
import time
from torch.utils.data import Dataset, DataLoader, random_split, Subset
# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞, –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤:
from torch.amp import autocast, GradScaler

# ========== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê GPU ==========
print("=" * 60)
print("ü§ñ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´ GOOGLE COLAB")
print("=" * 60)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"‚úÖ GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞: {torch.cuda.get_device_name(0)}")
    print(f"   –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"   CUDA: {torch.version.cuda}")
else:
    device = torch.device("cpu")
    print("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

print(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
print("-" * 60)

class Bottleneck(nn.Module):
    """Bottleneck block –¥–ª—è ResNet50/101/152"""
    expansion = 4  # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤ –≤ 4 —Ä–∞–∑–∞

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(Bottleneck, self).__init__()

        # 1x1 —Å–≤—ë—Ä—Ç–∫–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        # 3x3 —Å–≤—ë—Ä—Ç–∫–∞
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # 1x1 —Å–≤—ë—Ä—Ç–∫–∞ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity  # Residual connection
        out = self.relu(out)

        return out


class ResNet50(nn.Module):
    """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è ResNet50 —Å –Ω—É–ª—è"""

    def __init__(self, num_classes=120, zero_init_residual=False):
        super(ResNet50, self).__init__()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ResNet50
        self.in_channels = 64

        # –ù–∞—á–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNet —Å–ª–æ–∏
        self.layer1 = self._make_layer(Bottleneck, 64, 3, stride=1)
        self.layer2 = self._make_layer(Bottleneck, 128, 4, stride=2)
        self.layer3 = self._make_layer(Bottleneck, 256, 6, stride=2)
        self.layer4 = self._make_layer(Bottleneck, 512, 3, stride=2)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights(zero_init_residual)

    def _make_layer(self, block, out_channels, blocks, stride=1):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ—è ResNet"""
        downsample = None

        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã (stride != 1 –∏–ª–∏ –∫–∞–Ω–∞–ª—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç)
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * block.expansion)
            )

        layers = []
        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫ –≤ —Å–ª–æ–µ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å downsample
        layers.append(block(self.in_channels, out_channels, stride, downsample))

        self.in_channels = out_channels * block.expansion

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))

        return nn.Sequential(*layers)

    def _initialize_weights(self, zero_init_residual):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π ResNet"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                  nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # –î–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è std –±–æ–ª—å—à–µ
                if m is self.fc:
                    nn.init.normal_(m.weight, 0, 0.1)  # std=0.1 –¥–ª—è 120 –∫–ª–∞—Å—Å–æ–≤
                else:
                    nn.init.normal_(m.weight, 0, 0.01)  # std=0.01 –¥–ª—è –¥—Ä—É–≥–∏—Ö Linear
                nn.init.constant_(m.bias, 0)

        # Zero-initialize –ø–æ—Å–ª–µ–¥–Ω–∏–π BatchNorm –≤ –∫–∞–∂–¥–æ–º residual branch
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)

    def forward(self, x):
        # –ù–∞—á–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        # ResNet —Å–ª–æ–∏
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x

def setup_environment():
    # –í Colab –¥–∞—Ç–∞—Å–µ—Ç –±—É–¥–µ—Ç –≤ /content –ø–æ—Å–ª–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    # –ù–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞–∫–æ–π: "C:/Users/1/.cache/kagglehub/datasets/jessicali9530/stanford-dogs-dataset/versions/2/images"
    data_dir = "/content/dogs"  # –ü—É—Ç—å –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU
    if torch.cuda.is_available():
        print(f"üìä –ü–∞–º—è—Ç—å GPU –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    return device, data_dir


def prepare_dataloaders_smart(data_dir, batch_size=256, max_images=12000):
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    IMAGENET_MEAN = [0.485, 0.456, 0.406]
    IMAGENET_STD = [0.229, 0.224, 0.225]

    # –î–ª—è –º–æ–¥–µ–ª–∏ ResNet50 —Å –Ω—É–ª—è –ø–æ–º–µ–Ω—è–ª–∏ Resize —Å 256 –¥–æ 512
    # –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø–æ–º–µ–Ω—è–ª–∏ CenterCrop —Å 224 –Ω–∞ 448 –¥–ª—è —ç—Ç–æ–π –∂–µ –Ω–µ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    # –î–ª—è –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Resize –∏ CenterCrop –±—ã–ª–∏ –¥—Ä—É–≥–∏–º–∏
    transform_train = transforms.Compose([
        transforms.Resize(512),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(5),
        transforms.CenterCrop(448),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])

    transform_test = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    base_dataset = ImageFolder(os.path.join(data_dir, 'Images'))

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    if len(base_dataset) > max_images:
        indices = torch.randperm(len(base_dataset))[:max_images]
        base_dataset = Subset(base_dataset, indices)

    # –†–∞–∑–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
    train_size = int(0.7 * len(base_dataset))
    val_size = int(0.15 * len(base_dataset))
    test_size = len(base_dataset) - train_size - val_size

    generator = torch.Generator().manual_seed(42)
    train_indices, val_indices, test_indices = random_split(
        range(len(base_dataset)),
        [train_size, val_size, test_size],
        generator=generator
    )

    # –ö–∞—Å—Ç–æ–º–Ω—ã–π DatasetWrapper
    class DatasetWrapper(Dataset):
        def __init__(self, base_dataset, indices, transform):
            self.base_dataset = base_dataset
            self.indices = list(indices)
            self.transform = transform

        def __len__(self):
            return len(self.indices)

        def __getitem__(self, idx):
            real_idx = self.indices[idx]
            img, label = self.base_dataset[real_idx]
            if self.transform:
                img = self.transform(img)
            return img, label

    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞
    train_dataset = DatasetWrapper(base_dataset, train_indices.indices, transform_train)
    val_dataset = DatasetWrapper(base_dataset, val_indices.indices, transform_test)
    test_dataset = DatasetWrapper(base_dataset, test_indices.indices, transform_test)

    # DataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers = True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers = True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers = True)

    return train_loader, val_loader, test_loader, base_dataset.dataset.classes


def create_model(pretrained=True, num_classes=120, device=None):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print(f"\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ {device}")

    if pretrained:
        from torchvision.models import ResNet50_Weights
        model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        print(" –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è ResNet50")
    else:
        #model = models.resnet50(weights=None)
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è ResNet50 —Å –Ω—É–ª—è
        model = ResNet50(num_classes = num_classes)
        print(" ResNet50 —Å –Ω—É–ª—è")

    # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model = model.to(device)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞
    print(f"üìè –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
    print(f"üìç –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {next(model.parameters()).device}")

    return model


def calculate_metrics(model, data_loader, device):
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫"""
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)

    return accuracy, precision, recall


def train_model_unified(model, train_loader, val_loader, device,
                               use_swa=False, num_epochs=30, is_pretrained=True):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º fine-tuning –∏ Mixed Precision
    –¢–æ–ª—å–∫–æ StepLR, –±–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ –∞–Ω–Ω–∏–≥–∏–Ω–≥–∞
    –ü–µ—Ä–≤—ã–µ —Å–ª–æ–∏ (conv1, layer1, layer2) –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    """

    criterion = nn.CrossEntropyLoss().to(device)

    # –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:
    print(f"Model device: {next(model.parameters()).device}")
    print(f"Batch size: {train_loader.batch_size}")

    # Mixed Precision Training –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è GPU
    scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None
    print(f"   Mixed Precision: {'–í–ö–õ–Æ–ß–ï–ù–û' if scaler else '–û–¢–ö–õ–Æ–ß–ï–ù–û (–Ω–µ—Ç GPU)'}")

    if is_pretrained:
        print("=" * 60)
        print("–†–ï–ñ–ò–ú: Fine-tuning –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
        print("   –ü–µ—Ä–≤—ã–µ —Å–ª–æ–∏ (conv1, layer1, layer2) –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã")
        print("=" * 60)

        # === –®–ê–ì 1: –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –í–°–ï —Å–ª–æ–∏ ===
        for param in model.parameters():
            param.requires_grad = False

        # === –§–ê–ó–ê 1: –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (8 —ç–ø–æ—Ö) ===
        print("\n–§–∞–∑–∞ 1: –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (8 —ç–ø–æ—Ö)")
        print("   –û–±—É—á–∞–µ—Ç—Å—è: fc (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä)")
        print("   –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã")

        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –¢–û–õ–¨–ö–û –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        for param in model.fc.parameters():
            param.requires_grad = True

        optimizer = optim.Adam(model.fc.parameters(), lr=0.002, weight_decay=0.01)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.65)

        for epoch in range(8):
            model.train()
            total_loss = 0
            epoch_start = time.time()

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
                optimizer.zero_grad()

                # Mixed precision forward
                if scaler:
                    with torch.amp.autocast('cuda'):
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    # Mixed precision backward
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()

                total_loss += loss.item()

            scheduler.step()
            epoch_time = time.time() - epoch_start
            val_acc, _, _ = calculate_metrics(model, val_loader, device)

            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                print(f"   –≠–ø–æ—Ö–∞ {epoch+1:2d}: "
                      f"Loss={total_loss/len(train_loader):.4f}, "
                      f"Val Acc={val_acc:.4f}, "
                      f"–í—Ä–µ–º—è={epoch_time:.1f}—Å, "
                      f"GPU={memory_used:.2f}GB")
            else:
                print(f"   –≠–ø–æ—Ö–∞ {epoch+1:2d}: "
                      f"Loss={total_loss/len(train_loader):.4f}, "
                      f"Val Acc={val_acc:.4f}, "
                      f"–í—Ä–µ–º—è={epoch_time:.1f}—Å")

        # === –§–ê–ó–ê 2: –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º layer4 (10 —ç–ø–æ—Ö) ===
        print("\n–§–∞–∑–∞ 2: –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º layer4 (10 —ç–ø–æ—Ö)")
        print("   –û–±—É—á–∞—é—Ç—Å—è: layer4, fc")
        print("   –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ã: conv1, layer1, layer2, layer3")

        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –¢–û–õ–¨–ö–û layer4
        for param in model.layer4.parameters():
            param.requires_grad = True

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è layer4 + fc
        trainable_params = filter(lambda p: p.requires_grad, model.parameters())
        optimizer = optim.Adam([
            {'params': model.fc.parameters(), 'lr': 0.0001},
            {'params': model.layer4.parameters(), 'lr': 0.00002}  # –í 5 —Ä–∞–∑ –º–µ–Ω—å—à–µ!
        ], weight_decay=0.001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.75)

        for epoch in range(10):
            model.train()
            total_loss = 0
            epoch_start = time.time()

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
                optimizer.zero_grad()

                # Mixed precision
                if scaler:
                    with torch.amp.autocast('cuda'):
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()

                total_loss += loss.item()

            scheduler.step()
            epoch_time = time.time() - epoch_start
            val_acc, _, _ = calculate_metrics(model, val_loader, device)

            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                print(f"   –≠–ø–æ—Ö–∞ {epoch+9:2d}: "
                      f"Loss={total_loss/len(train_loader):.4f}, "
                      f"Val Acc={val_acc:.4f}, "
                      f"–í—Ä–µ–º—è={epoch_time:.1f}—Å, "
                      f"GPU={memory_used:.2f}GB")
            else:
                print(f"   –≠–ø–æ—Ö–∞ {epoch+9:2d}: "
                      f"Loss={total_loss/len(train_loader):.4f}, "
                      f"Val Acc={val_acc:.4f}, "
                      f"–í—Ä–µ–º—è={epoch_time:.1f}—Å")

        # === –§–ê–ó–ê 3: –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º layer3 (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ) ===
        dataset_size = len(train_loader.dataset.base_dataset)
        remaining_epochs = num_epochs - 18

        if dataset_size > 10000:  # –î–ª—è Stanford Dogs (20k) - —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º
            print(f"\n–§–∞–∑–∞ 3: –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º layer3 ({remaining_epochs} —ç–ø–æ—Ö)")
            print("   –û–±—É—á–∞—é—Ç—Å—è: layer3, layer4, fc")
            print("   –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ã: conv1, layer1, layer2")

            # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º layer3
            for param in model.layer3.parameters():
                param.requires_grad = True

            # –ù–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è layer3 + layer4 + fc
            trainable_params = filter(lambda p: p.requires_grad, model.parameters())
            optimizer = optim.Adam(trainable_params, lr=0.0001, weight_decay=0.01)  # –ú–µ–Ω—å—à–∏–π LR –¥–ª—è layer3
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.85)

        else:
            print(f"\n–§–∞–∑–∞ 3: –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ layer4 + fc ({remaining_epochs} —ç–ø–æ—Ö)")
            print("   –û–±—É—á–∞—é—Ç—Å—è: layer4, fc")
            print("   –ó–∞–º–æ—Ä–æ–∂–µ–Ω—ã: conv1, layer1, layer2, layer3")
            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –æ—Å—Ç–∞—ë—Ç—Å—è —Ç–æ—Ç –∂–µ (—Ç–æ–ª—å–∫–æ layer4 + fc)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.85)

    else:  # –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è
        print("=" * 50)
        print("–†–ï–ñ–ò–ú: –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
        print("=" * 50)

        for param in model.parameters():
            param.requires_grad = True

        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
        remaining_epochs = num_epochs

    # === –û–ë–©–ê–Ø –§–ê–ó–ê –û–ë–£–ß–ï–ù–ò–Ø ===
    print(f"\n–û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ({remaining_epochs} —ç–ø–æ—Ö)")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫–∏–µ —Å–ª–æ–∏ –æ–±—É—á–∞—é—Ç—Å—è
    trainable_layers = []
    for name, param in model.named_parameters():
        if param.requires_grad and name.split('.')[0] not in trainable_layers:
            trainable_layers.append(name.split('.')[0])

    print(f"   –û–±—É—á–∞–µ–º—ã–µ —Å–ª–æ–∏: {', '.join(sorted(set(trainable_layers)))}")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SWA
    swa_model = None
    if use_swa:
        swa_model = AveragedModel(model).to(device)
        swa_start = int(remaining_epochs * 0.7)  # –ù–∞—á–∏–Ω–∞–µ–º —Å 70% —ç–ø–æ—Ö
        print(f"   SWA –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è —Å —ç–ø–æ—Ö–∏ {swa_start}")

    for epoch in range(remaining_epochs):
        model.train()
        total_loss = 0
        epoch_start = time.time()

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            optimizer.zero_grad()

            # Mixed precision
            if scaler:
                with torch.amp.autocast('cuda'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

            total_loss += loss.item()

        epoch_time = time.time() - epoch_start

        # SWA –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        if use_swa and swa_model and (epoch >= swa_start):
            swa_model.update_parameters(model)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ LR
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_acc, _, _ = calculate_metrics(model, val_loader, device)

        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            print(f"   –≠–ø–æ—Ö–∞ {epoch+1:3d}: "
                  f"Loss={total_loss/len(train_loader):.4f}, "
                  f"Val Acc={val_acc:.4f}, "
                  f"LR={current_lr:.6f}, "
                  f"–í—Ä–µ–º—è={epoch_time:.1f}—Å, "
                  f"GPU={memory_used:.2f}GB")
        else:
            print(f"   –≠–ø–æ—Ö–∞ {epoch+1:3d}: "
                  f"Loss={total_loss/len(train_loader):.4f}, "
                  f"Val Acc={val_acc:.4f}, "
                  f"LR={current_lr:.6f}, "
                  f"–í—Ä–µ–º—è={epoch_time:.1f}—Å")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ SWA
    if use_swa and swa_model:
        torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)
        print("‚úÖ SWA –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞")
        return swa_model

    return model


def main():
    print("=" * 60)
    print(" –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –û–ë–£–ß–ï–ù–ò–Ø - Stanford Dogs")
    print("=" * 60)

    total_start = time.time()
    device, data_dir = setup_environment()
    BATCH_SIZE = 64 # –î–ª—è –Ω–µ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ

    train_loader, val_loader, test_loader, classes = prepare_dataloaders_smart(data_dir, batch_size=BATCH_SIZE, max_images=12000)

    #–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è + Adam (10 —ç–ø–æ—Ö)
    print("\n" + "=" * 50)
    print(" –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è + Adam")
    print("=" * 50)

    exp1_start = time.time()
    model1 = create_model(pretrained=True, num_classes=120, device=device)
    model1 = train_model_unified(model1, train_loader, val_loader, device,
                                 use_swa=False, num_epochs=50, is_pretrained=True)

    # Train –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Train –≤—ã–±–æ—Ä–∫–∞:")
    train_acc1, train_prec1, train_rec1 = calculate_metrics(model1, train_loader, device)
    print(f"  Accuracy:  {train_acc1:.4f}")
    print(f"  Precision: {train_prec1:.4f}")
    print(f"  Recall:    {train_rec1:.4f}")

    # Validation –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Validation –≤—ã–±–æ—Ä–∫–∞:")
    val_acc1, val_prec1, val_rec1 = calculate_metrics(model1, val_loader, device)
    print(f"  Accuracy:  {val_acc1:.4f}")
    print(f"  Precision: {val_prec1:.4f}")
    print(f"  Recall:    {val_rec1:.4f}")

    # Test –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Test –≤—ã–±–æ—Ä–∫–∞:")
    test_acc1, test_prec1, test_rec1 = calculate_metrics(model1, test_loader, device)
    print(f"  Accuracy:  {test_acc1:.4f}")
    print(f"  Precision: {test_prec1:.4f}")
    print(f"  Recall:    {test_rec1:.4f}")

    #–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è + Averaged Adam (10 —ç–ø–æ—Ö)
    print("\n" + "=" * 50)
    print(" –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è + Averaged Adam")
    print("=" * 50)

    exp2_start = time.time()
    model2 = create_model(pretrained=True)
    model2 = train_model_unified(model2, train_loader, val_loader, device,
                                 use_swa=True, num_epochs=50, is_pretrained=True)

    # Train –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Train –≤—ã–±–æ—Ä–∫–∞:")
    train_acc2, train_prec2, train_rec2 = calculate_metrics(model2, train_loader, device)
    print(f"  Accuracy:  {train_acc2:.4f}")
    print(f"  Precision: {train_prec2:.4f}")
    print(f"  Recall:    {train_rec2:.4f}")

    # Validation –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Validation –≤—ã–±–æ—Ä–∫–∞:")
    val_acc2, val_prec2, val_rec2 = calculate_metrics(model2, val_loader, device)
    print(f"  Accuracy:  {val_acc2:.4f}")
    print(f"  Precision: {val_prec2:.4f}")
    print(f"  Recall:    {val_rec2:.4f}")

    # Test –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Test –≤—ã–±–æ—Ä–∫–∞:")
    test_acc2, test_prec2, test_rec2 = calculate_metrics(model2, test_loader, device)
    print(f"  Accuracy:  {test_acc2:.4f}")
    print(f"  Precision: {test_prec2:.4f}")
    print(f"  Recall:    {test_rec2:.4f}")

    #–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: –° –Ω—É–ª—è + Adam (10 —ç–ø–æ—Ö)
    print("\n" + "=" * 50)
    print(" –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 3: –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è + Adam")
    print("=" * 50)
    print(" –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: ResNet50 —Å –Ω—É–ª—è —Ç—Ä–µ–±—É–µ—Ç 50+ —ç–ø–æ—Ö")

    exp3_start = time.time()
    model3 = create_model(pretrained=False)
    model3 = train_model_unified(model3, train_loader, val_loader, device,
                                 use_swa=False, num_epochs=80, is_pretrained=False)

    # Train –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Train –≤—ã–±–æ—Ä–∫–∞:")
    train_acc3, train_prec3, train_rec3 = calculate_metrics(model3, train_loader, device)
    print(f"  Accuracy:  {train_acc3:.4f}")
    print(f"  Precision: {train_prec3:.4f}")
    print(f"  Recall:    {train_rec3:.4f}")

    # Validation –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Validation –≤—ã–±–æ—Ä–∫–∞:")
    val_acc3, val_prec3, val_rec3 = calculate_metrics(model3, val_loader, device)
    print(f"  Accuracy:  {val_acc3:.4f}")
    print(f"  Precision: {val_prec3:.4f}")
    print(f"  Recall:    {val_rec3:.4f}")

    # Test –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä Test –≤—ã–±–æ—Ä–∫–∞:")
    test_acc3, test_prec3, test_rec3 = calculate_metrics(model3, test_loader, device)
    print(f"  Accuracy:  {test_acc3:.4f}")
    print(f"  Precision: {test_prec3:.4f}")
    print(f"  Recall:    {test_rec3:.4f}")

    total_time = (time.time() - total_start) / 60
    print(f"\n –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –ó–ê {total_time:.1f} –ú–ò–ù–£–¢")

    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ GPU
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"\nüßπ –ü–∞–º—è—Ç—å GPU –æ—á–∏—â–µ–Ω–∞")


if __name__ == "__main__":
    main()